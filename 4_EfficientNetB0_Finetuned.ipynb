{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08fb33e",
   "metadata": {},
   "source": [
    "# 4.0 EfficientNetB0 Model (Finetuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938a709",
   "metadata": {},
   "source": [
    "This Jupyter Notebook contains the code required to finetune the EfficientNetB0 model on the training dataset and evaluate its performance on the validation dataset. The notebook is divided into eight sections, each containing code for a specific task.\n",
    "\n",
    "The notebook is structured into eight main sections:\n",
    "\n",
    "1. **Importing Required Libraries:** This section imports the necessary libraries for the notebook, including TensorFlow, NumPy, Matplotlib, and Scikit-learn.\n",
    "\n",
    "2. **Mapping Imagenet Class Index to Human-Readable Labels:** This section maps the ImageNet class index to human-readable labels, which is necessary for interpreting the results of the EfficientNetB0 model predictions.\n",
    "\n",
    "3. **Preparing and Loading the Training and Validation Datasets:** This section prepares and loads the training and validation datasets for input to the model.\n",
    "\n",
    "4. **Training the Model:** This section defines the EfficientNetB0 model architecture, compiles the model, and trains it on the training dataset.\n",
    "\n",
    "5. **Plotting Training Log:** This section plots the training and validation accuracy and loss over the training epochs.\n",
    "\n",
    "6. **Obtaining Predictions for Validation Set:** This section makes predictions on the validation dataset using the trained EfficientNetB0 model.\n",
    "\n",
    "7. **Calculating Performance Metrics:** This section calculates performance metrics for the trained EfficientNetB0 model using Scikit-learn. It calculates the precision score, recall score, F1 score, and accuracy score for the predicted labels.\n",
    "\n",
    "8. **Plotting Confusion Matrix:** This section plots the confusion matrix for the predicted and actual labels.\n",
    "\n",
    "Furthermore, this notebook provides a comprehensive evaluation of the finetuned EfficientNetB0 model's performance on the validation dataset and provides insights into the model's strengths and weaknesses which will be compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce5e927",
   "metadata": {},
   "source": [
    "## 4.1 Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aac3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9f8875",
   "metadata": {},
   "source": [
    "## 4.2 Mapping Imagenet Class Index to Human-Readable Labels\n",
    "\n",
    "> The ImageNet metadata JSON file was obtained from: https://www.kaggle.com/keras/resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e811a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary\n",
    "imagenet2idx = {}\n",
    "\n",
    "# Open the file 'imagenet_class_index.json' in read mode and assign its content to the dictionary\n",
    "with open('imagenet_class_index.json') as f:\n",
    "    idx2imagenet = json.load(f)\n",
    "    \n",
    "# Print the contents\n",
    "idx2imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dictionary with keys and values from 'idx2imagenet'\n",
    "# The keys are the first elements of the values in 'idx2imagenet' and the values are a list of the \n",
    "# corresponding key in 'idx2imagenet' and the second element of the values in 'idx2imagenet'\n",
    "imagenet2idx = {v[0]: [k, v[1]] for k, v in idx2imagenet.items()}\n",
    "\n",
    "# Print the contents\n",
    "imagenet2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d1056",
   "metadata": {},
   "source": [
    "## 4.3 Preparing and Loading the Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths to the directories containing the training and validation image datasets\n",
    "TRAIN_PATH = \"imageset/train\"\n",
    "VAL_PATH = \"imageset/val\"\n",
    "\n",
    "# Set the batch size, image size, and number of epochs for training the model\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224,224)\n",
    "EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image data into two `tf.data.Dataset` objects using `image_dataset_from_directory()`\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    VAL_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='int',\n",
    "    seed=123,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0787959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of `class_names` by iterating through the `train_ds` `class_names` attribute\n",
    "class_names = []\n",
    "for class_name in train_ds.class_names:\n",
    "    class_names.append(imagenet2idx[class_name][1])\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the `AUTOTUNE` constant to automatically determine an optimal buffer size for `prefetch()`\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Cache, shuffle, and prefetch the training and validation data for better performance\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce75042c",
   "metadata": {},
   "source": [
    "## 4.4 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6447454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input layer with shape\n",
    "ip = tf.keras.layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "\n",
    "# Create base model with EfficientNetB0 architecture, pretrained weights, and global average pooling\n",
    "base_model = EfficientNetB0(include_top=False, input_tensor=ip, weights=\"imagenet\", pooling='avg')(ip)\n",
    "\n",
    "# Set base model to not be trainable\n",
    "base_model.trainable = False\n",
    "\n",
    "# Apply batch normalisation to output of base model\n",
    "x = tf.keras.layers.BatchNormalization()(base_model)\n",
    "\n",
    "# Apply dropout regularisation to output of batch normalisation\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add final dense layer with softmax activation to output predicted class probabilities\n",
    "op = tf.keras.layers.Dense(len(class_names), activation=\"softmax\", name=\"pred\")(x)\n",
    "\n",
    "# Create the overall model by specifying the input and output layers\n",
    "model = tf.keras.Model(ip, op)\n",
    "\n",
    "# Compile model with Adam optimizer, sparse categorical cross-entropy loss, and accuracy metric\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print summary of model architecture and number of parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a ModelCheckpoint callback to save the best model during training, based on the validation loss\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"models/best_model_finetune.h5\", \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='auto', \n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "# Train the model using the training dataset for a specified number of epochs, with the validation dataset \n",
    "# used for evaluation\n",
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=val_ds, \n",
    "    verbose=1, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6cbc30",
   "metadata": {},
   "source": [
    "## 4.5 Plotting Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b277604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the default seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set the context for plotting\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a new figure with a size of 9 inches x 5 inches\n",
    "fig = plt.figure(figsize=(9,5))\n",
    "\n",
    "# Add a subplot to the figure with a grid of 1 row and 2 columns, and select the first column\n",
    "fig.add_subplot(1,2,1)\n",
    "\n",
    "# Plot the training and validation accuracy over the epochs\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Add a subplot to the figure with a grid of 1 row and 2 columns, and select the second column\n",
    "fig.add_subplot(1,2,2)\n",
    "\n",
    "# Plot the training and validation loss over the epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "# Adjust the subplot layout for better spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the figure to a PNG file\n",
    "plt.savefig(\"figures/model_performance_finetuned.png\", dpi=300)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724ca04",
   "metadata": {},
   "source": [
    "## 4.6 Obtaining Predictions for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff96979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all JPEG image files in the validation directory and store the paths in a list\n",
    "val_images_list = glob.glob(os.path.join(VAL_PATH, \"**\", \"*.JPEG\"), recursive=True)\n",
    "\n",
    "# Initialise empty lists for ground truth and predicted labels\n",
    "actual_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Loop through each validation image path\n",
    "for val_image_path in val_images_list:\n",
    "    \n",
    "    # Extract the ground truth label by finding the index of the class name\n",
    "    # in a list of class names, using a dictionary to map from image file names to class names\n",
    "    actual_labels.append(class_names.index(imagenet2idx[val_image_path.split(\"\\\\\")[1]][1]))\n",
    "    \n",
    "    # Load the image, resize it to a target size, and convert it to a NumPy array\n",
    "    img = image.load_img(val_image_path, target_size=IMG_SIZE)\n",
    "    x = np.expand_dims(image.img_to_array(img), axis=0)\n",
    "\n",
    "    # Predict class probabilities for input image and append predicted label index to list\n",
    "    preds = model.predict(x, verbose=0)\n",
    "    predicted_labels.append(np.argmax(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454d186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print ground truth labels\n",
    "print(\"Ground truth labels:\")\n",
    "actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72533a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print predicted labels\n",
    "print(\"Predicted labels:\")\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a75706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print class name\n",
    "print(\"Class names:\")\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed3931",
   "metadata": {},
   "source": [
    "## 4.7 Calculating Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f71a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision score for the predicted labels\n",
    "print(\"Precison:\")\n",
    "precision_score(actual_labels, predicted_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b59a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recall score for the predicted labels\n",
    "print(\"Recall score:\")\n",
    "recall_score(actual_labels, predicted_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate f1 score for the predicted labels\n",
    "print(\"F1 score:\")\n",
    "f1_score(actual_labels, predicted_labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37684a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy score for the predicted labels\n",
    "print(\"Accuracy score:\")\n",
    "accuracy_score(actual_labels, predicted_labels, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dcc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a classification report\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(actual_labels, predicted_labels, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3c28a",
   "metadata": {},
   "source": [
    "## 4.8 Plotting Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn default theme\n",
    "sns.set_theme()\n",
    "\n",
    "# Set seaborn plot context\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Create a new matplotlib figure object with size 7x5 inches\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "\n",
    "# Calculate the confusion matrix for the predicted and actual labels\n",
    "cm = confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "# Create a pandas DataFrame from the confusion matrix\n",
    "cm_df = pd.DataFrame(cm, index=[i for i in class_names], columns=[i for i in class_names])\n",
    "\n",
    "# Create a heatmap of the confusion matrix using seaborn\n",
    "sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Confusion Matrix for Val Split')  \n",
    "\n",
    "# Adjust the layout of the plot\n",
    "fig.tight_layout()  \n",
    "\n",
    "# Save the figure to a PNG file\n",
    "plt.savefig(\"figures/confusion_matrix_finetuned.png\", dpi=300)  \n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf5015",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------\n",
    "\n",
    "#### Code adapted from:\n",
    "\n",
    "https://keras.io/api/applications/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/images/classification\n",
    "\n",
    "https://keras.io/examples/vision/image_classification_from_scratch/\n",
    "\n",
    "https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
    "\n",
    "https://www.kaggle.com/code/arjunrao2000/beginners-guide-efficientnet-with-keras/notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
